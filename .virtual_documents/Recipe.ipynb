import pandas as pd
import re


# Read the CSV file into a DataFrame
df = pd.read_csv("data/raw/recipes_sample.csv")


# print all columns and verify if any missing values
df.info()


# Drop all columns that is not required for analysis
clean_df = df.drop(["AuthorName", "AuthorId", "DatePublished", "CookTime", "PrepTime"], axis=1)


# Drop rows with NA values in 'AggregatedRating' and 'ReviewCount' columns where no reviews
clean_df.dropna(subset=['AggregatedRating', 'ReviewCount'], inplace=True)


# Splitting the 'RecipeYield' column into two new columns
clean_df[['Serving', 'Unit']] = clean_df['RecipeYield'].str.split(' ', n=1, expand=True)

# Converting 'Serving' column to integer
clean_df['Serving'] = clean_df['Serving'].str.extract('(\d+)').astype(float)

# Dropping the original 'RecipeYield' column and the 'Unit' column
clean_df.drop(['RecipeYield', 'Unit'], axis=1, inplace=True)


# Fill NA in "RecipeServings" with "Serving" value where "RecipeServings" is NA and "Serving" is not null
clean_df.loc[clean_df['RecipeServings'].isna() & clean_df['Serving'].notna(), 'RecipeServings'] = clean_df['Serving']

# Convert 'RecipeServings' column to integer type if necessary
clean_df['RecipeServings'] = clean_df['RecipeServings'].astype(float)


# Fill NA in "Keywords" with values from "RecipeIngredientParts"
clean_df['Keywords'].fillna(clean_df['RecipeIngredientParts'], inplace=True)


clean_df = clean_df.drop(["Serving"], axis=1)


# Fill NA in "RecipeServings" with 1
clean_df['RecipeServings'].fillna(1, inplace=True)


# Remove rows with NA in "RecipeCategory"
clean_df = clean_df.dropna(subset=['RecipeCategory', 'Description', 'RecipeIngredientQuantities'])

# Veirfy all column value are cleaned
clean_df.info()


# Remove "PT" prefix
clean_df['TotalTime'] = clean_df['TotalTime'].str.replace("PT", "")
clean_df.head()


# Create a new column "URL" with the initial value from "Name" and "RecipeId" column
# Food.com url example "https://www.food.com/recipe/japanese-pumpkin-soup-kabocha-soup-88935"

#Create new column URL to store generated URL Link
clean_df['URL'] = clean_df['Name']

# Remove parentheses, dashes, and commas from the "URL" column
clean_df['URL'] = clean_df['URL'].str.replace(r'[\(\)\-,]', '')  

# Select the "URL" column and apply transformations
clean_df['URL'] = clean_df['URL'].str.replace(' ', '-')  # Replace spaces with hyphens
clean_df['URL'] = clean_df['URL'].str.lower()  # Convert text to lowercase

# Concatenate "https://www.food.com/recipe/" with the modified "URL" column
clean_df['URL'] = 'https://www.food.com/recipe/' + clean_df['URL'] + '-' + clean_df['RecipeId'].astype(str)
clean_df['URL']


# Remove "c()" at the beginning and parentheses from each value
clean_df['Keywords'] = clean_df['Keywords'].str.replace(r'^c|[()]', '', regex=True)
clean_df['RecipeIngredientQuantities'] = clean_df['RecipeIngredientQuantities'].str.replace(r'^c|[()]', '', regex=True)
clean_df['RecipeIngredientParts'] = clean_df['RecipeIngredientParts'].str.replace(r'^c|[()]', '', regex=True)
clean_df['RecipeInstructions'] = clean_df['RecipeInstructions'].str.replace(r'^c|[()]', '', regex=True)
clean_df['Images'] = clean_df['Images'].str.replace(r'^c|[()]', '', regex=True)


# Split the strings by comma, explode the resulting lists, and get unique items
keyword_item = clean_df['Keywords'].str.split(',').explode().str.strip().unique()

# Convert to list
keyword_item_list = keyword_item.tolist()

# Print the length of the list
print("Number of unique items:", len(keyword_item_list))


# Create unique_keyword_item to understand what is in the Keywords column for future programming use
# Clean up each string (remove leading/trailing spaces, convert to lowercase, and strip double quotes) before converting to set
unique_keyword_item_set = set(map(lambda x: x.strip().lower().strip('"'), keyword_item_list))

# Convert the set back to a list if needed
unique_keyword_item_list = list(unique_keyword_item_set)

# Print the length of the list
print("Number of unique items:", len(unique_keyword_item_list))


# Verify all data are cleaned properly 
clean_df.head()


# Extract the recipe ingredients from the "RecipeIngredientParts" column
ingredients_series = clean_df['RecipeIngredientParts']

# Split the ingredients into individual items
ingredients_split = ingredients_series.str.split(',')

# Create a list of unique ingredients
unique_ingredients = []
for ingredients in ingredients_split:
    if isinstance(ingredients, list):
        for ingredient in ingredients:
            ingredient = ingredient.strip()
            if ingredient not in unique_ingredients and ingredient != '':
                unique_ingredients.append(ingredient)


# Create a DataFrame with columns representing unique ingredients
ingredient_df = pd.DataFrame(0, index=clean_df.index, columns=unique_ingredients)

# Populate the DataFrame with 1 where ingredient is present in the recipe
for ingredient in unique_ingredients:
    ingredient_df[ingredient] = ingredients_series.str.contains(ingredient, na=False, regex=False).astype(int)



# Create new table call recipes with clean_df and ingredient_df for ML features to learn
recipes = pd.concat([clean_df, ingredient_df], axis=1)


# Melt the DataFrame to convert ingredient columns into rows
melted_df = recipes.melt(var_name='Ingredient', value_name='Present')

# Filter out rows where ingredient is present
present_ingredients = melted_df[melted_df['Present'] == 1]

# Count the occurrences of each ingredient
ingredient_counts = present_ingredients['Ingredient'].value_counts().reset_index()

# Rename columns
ingredient_counts.columns = ['Ingredient', 'Count']


# Define the thresholds for each category to match each Healthy category
category_thresholds = {
    'low sodium': 140,    # Daily recommended intake for low sodium
    'low calorie': 200,   # Arbitrary threshold for low calorie
    'high protein': 20,   # Arbitrary threshold for high protein
    'low fat': 3,         # Arbitrary threshold for low fat
    'high fat': 20,       # Arbitrary threshold for high fat
    'high fiber': 5,      # Arbitrary threshold for high fiber
    'low carb': 20,       # Arbitrary threshold for low carb
    'low cholesterol': 20, # Arbitrary threshold for low cholesterol
    'low sugar': 5        # Arbitrary threshold for low sugar
}


# Create new columns for each category and assign boolean values based on thresholds
for category, threshold in category_thresholds.items():
    recipes[category] = False
    if category == 'low sodium':
        recipes.loc[recipes['SodiumContent'] < threshold, category] = True
    elif category == 'low calorie':
        recipes.loc[recipes['Calories'] < threshold, category] = True
    elif category == 'high protein':
        recipes.loc[recipes['ProteinContent'] > threshold, category] = True
    elif category == 'low fat':
        recipes.loc[recipes['FatContent'] < threshold, category] = True
    elif category == 'high fat':
        recipes.loc[recipes['FatContent'] > threshold, category] = True
    elif category == 'high fiber':
        recipes.loc[recipes['FiberContent'] > threshold, category] = True
    elif category == 'low carb':
        recipes.loc[recipes['CarbohydrateContent'] < threshold, category] = True
    elif category == 'low cholesterol':
        recipes.loc[recipes['CholesterolContent'] < threshold, category] = True
    elif category == 'low sugar':
        recipes.loc[recipes['SugarContent'] < threshold, category] = True


# Drop all columns except the newly created category columns
columns_to_keep = ["Name"] + list(category_thresholds.keys())

# make new copy table recipes2 to store binery of each category_thresholds
recipes2 = recipes[columns_to_keep]


# Function to determine the category for each recipe
def determine_category(row):
    for category, threshold in category_thresholds.items():
        if category == 'low sodium':
            if row['SodiumContent'] < threshold:
                return category
        elif category == 'low calorie':
            if row['Calories'] < threshold:
                return category
        elif category == 'high protein':
            if row['ProteinContent'] > threshold:
                return category
        elif category == 'low fat':
            if row['FatContent'] < threshold:
                return category
        elif category == 'high fat':
            if row['FatContent'] > threshold:
                return category
        elif category == 'high fiber':
            if row['FiberContent'] > threshold:
                return category
        elif category == 'low carb':
            if row['CarbohydrateContent'] < threshold:
                return category
        elif category == 'low cholesterol':
            if row['CholesterolContent'] < threshold:
                return category
        elif category == 'low sugar':
            if row['SugarContent'] < threshold:
                return category
    # If no category is assigned, return None
    return None

# Apply the determine_category function to each row to determine the category for each recipe and add it to the recipes table
recipes['AssignedCategory'] = recipes.apply(determine_category, axis=1)


# Verify all Category are properly appended
recipes.head()


# Adding Health Keywords to Keywords column
# Define the criteria for adding keywords
criteria = (
    recipes['FiberContent'] > 5
) | (
    recipes['RecipeCategory'].str.contains('High In', na=False)
) | (
    recipes['ProteinContent'] > 20
) | (
    recipes['RecipeCategory'].str.contains('Homeopathy/Remedies', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Kid Friendly', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Lactose Free', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Low Cholesterol', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Low Protein', na=False)
) | (
    recipes['RecipeCategory'].str.contains('No Shell Fish', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Soy/Tofu', na=False)
) | (
    recipes['CarbohydrateContent'] < 5
) | (
    recipes['RecipeCategory'].str.contains('High In.*Diabetic Friendly', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Bath/Beauty', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Egg Free', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Healthy', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Kosher', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Toddler Friendly', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Vegan', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Vegetable', na=False)
)


# Add the specified keywords to the Keywords column for recipes that meet the criteria
clean_df.loc[criteria, 'Keywords'] = recipes.loc[criteria, 'Keywords'] + ', "High Fiber", "High Protein", "Homeopathy/Remedies", "Kid Friendly", "Lactose Free", "Low Cholesterol", "Low Protein", "No Shell Fish", "Soy/Tofu", "Very Low Carbs", "Diabetic Friendly", "Bath/Beauty", "Egg Free", "Healthy", "Kosher", "Toddler Friendly", "Vegan", "Vegetable"'


# Get the column names of ingredient_df
ingredient_columns = ingredient_df.columns.tolist()

# Assign ingredient_columns to columns_to_drop
columns_to_drop = ingredient_columns

# Dropping the columns from the DataFrame
recipes.drop(columns=columns_to_drop, inplace=True)


# Save clean data to a new CSV file for ML model testing
recipes.to_csv("data/clean/ML_test_recipes.csv", index=False)


# Save clean data to a new JSON file
clean_df.to_json("data/clean/cleanSamplerecipes.json", orient="records")





# Importing necessary libraries
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
import matplotlib.pyplot as plt
import numpy as np
import joblib

# Read the ML test CSV file into a DataFrame
ML_test_df = pd.read_csv("data/clean/ML_test_recipes.csv")

# List of columns containing string values to be dropped
columns_to_drop = ['Name', 'TotalTime', 'Images', 'RecipeCategory', 'Keywords', 'Description',
                   'RecipeIngredientQuantities', 'RecipeIngredientParts', 'AggregatedRating','ReviewCount',
                  'RecipeInstructions', 'URL', 'AssignedCategory']           

# Dropping the columns from the DataFrame
ML_test_df.drop(columns=columns_to_drop, inplace=True)
ML_test_df.head()


# Define features (X) and target variable (y)
# Features
X = ML_test_df.drop(columns=['low sodium', 'low calorie', 'high protein', 'low fat', 'high fat',	
                             'high fiber', 'low carb', 'low cholesterol', 'low sugar'])
# Labels for multiple health attributes
y = ML_test_df[['low sodium', 'low calorie', 'high protein', 'low fat',	'high fat',	'high fiber', 
                'low carb', 'low cholesterol', 'low sugar']]  # Target variable

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Model Selection with Hyperparameter Tuning using GridSearchCV
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_

# Training the Best Model
best_model.fit(X_train, y_train)


# Model Evaluation
y_pred = best_model.predict(X_test)
print("Classification Report:")
print(classification_report(y_test, y_pred))


# Calculating the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy of Random Forest classifier:", accuracy)


# Class labels
classes = ['low sodium', 'low calorie', 'high protein', 'low fat', 'high fat', 'high fiber', 'low carb', 'low cholesterol', 'low sugar']

# Precision, recall, and F1-score values for each class
precision = [0.99, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00]
recall = [0.95, 0.99, 0.99, 0.88, 0.98, 0.91, 0.99, 1.00, 1.00]
f1_score = [0.97, 0.99, 0.99, 0.94, 0.99, 0.95, 0.99, 1.00, 1.00]

# Set width of bar
bar_width = 0.25

# Set position of bar on X axis
r1 = np.arange(len(precision))
r2 = [x + bar_width for x in r1]
r3 = [x + bar_width for x in r2]

# Make the plot
plt.figure(figsize=(12, 6))
plt.bar(r1, precision, color='b', width=bar_width, edgecolor='grey', label='Precision')
plt.bar(r2, recall, color='g', width=bar_width, edgecolor='grey', label='Recall')
plt.bar(r3, f1_score, color='r', width=bar_width, edgecolor='grey', label='F1-score')

# Add xticks on the middle of the group bars
plt.xlabel('Class', fontweight='bold')
plt.xticks([r + bar_width for r in range(len(precision))], classes, rotation=45)

# Add y label
plt.ylabel('Scores', fontweight='bold')

# Create legend & Show graphic
plt.legend()
plt.title('Classification Metrics by Class')
plt.tight_layout()
plt.show()


# Save the best model to a file
joblib.dump(best_model, 'data/MLmodel/best_model.pkl')





# Importing necessary libraries
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import multilabel_confusion_matrix
import joblib
import numpy as np
import matplotlib.pyplot as plt

# Read the ML test CSV file into a DataFrame
ML_test_df = pd.read_csv("data/clean/ML_test_recipes.csv")
ML_test_df.info()


# Reduce feature for faster process to predict by removed all ingredients from generate the test sample data 
# Get the column names of ingredient_df
ingredient_columns = ingredient_df.columns.tolist()

# Initialize an empty list to store columns that exist in ML_test_df
columns_to_drop_existing = []

# Check if each column exists in ML_test_df before adding it to the list of columns to drop
for column in columns_to_drop:
    if column in ML_test_df.columns:
        columns_to_drop_existing.append(column)

# Dropping the existing columns from the DataFrame
ML_test_df.drop(columns=columns_to_drop_existing, inplace=True)


# Define features (X) and target variable (y)
# Features
X = ML_test_df.drop(columns=['low sodium', 'low calorie', 'high protein', 'low fat', 'high fat',	
                             'high fiber', 'low carb', 'low cholesterol', 'low sugar'])
# Labels for multiple health attributes
y = ML_test_df[['low sodium', 'low calorie', 'high protein', 'low fat',	'high fat',	'high fiber', 
                'low carb', 'low cholesterol', 'low sugar']]  # Target variable

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Creating StandardScaler instance
scaler = StandardScaler()


# Fitting Standard Scaller
X_scaler = scaler.fit(X_train)


# Scaling data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)


# Create a random forest classifier
re_model = RandomForestClassifier(n_estimators = 100, random_state=42)


# Fitting the model
re_model = re_model.fit(X_train_scaled, y_train)


# Make Prediction useing the test data
predictions = re_model.predict(X_test_scaled)


# Calculate the confusion matrix
cm = multilabel_confusion_matrix(y_test.values.argmax(axis=1), predictions.argmax(axis=1))

# Create a DataFrame to display the confusion matrix
class_names = ['low sodium', 'low calorie', 'high protein', 'low fat', 'high fat', 'high fiber', 'low carb', 'low cholesterol', 'low sugar']
cm_df = pd.DataFrame(index=pd.MultiIndex.from_product([["Actual"], ["Predicted 0", "Predicted 1"]]), columns=class_names)

for i, class_name in enumerate(class_names):
    cm_df.loc[("Actual", "Predicted 0"), class_name] = cm[i, 0, 0]
    cm_df.loc[("Actual", "Predicted 1"), class_name] = cm[i, 1, 1]

# Calculate the accuracy score
acc_score = accuracy_score(y_test, predictions)


# Display results
print("Multilabel Confusion Matrix")
display(cm_df)
print(f"Accuracy Score : {acc_score}")
print("Classification Report")
print(classification_report(y_test, predictions))


# Class labels
classes = ['low sodium', 'low calorie', 'high protein', 'low fat', 'high fat', 'high fiber', 'low carb', 'low cholesterol', 'low sugar']

# Precision, recall, and F1-score values for each class
precision = [0.99, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00]
recall = [0.95, 0.99, 0.99, 0.88, 0.98, 0.91, 0.99, 1.00, 1.00]
f1_score = [0.97, 0.99, 0.99, 0.94, 0.99, 0.95, 0.99, 1.00, 1.00]

# Set width of bar
bar_width = 0.25

# Set position of bar on X axis
r1 = np.arange(len(precision))
r2 = [x + bar_width for x in r1]
r3 = [x + bar_width for x in r2]

# Make the plot
plt.figure(figsize=(12, 6))
plt.bar(r1, precision, color='b', width=bar_width, edgecolor='grey', label='Precision')
plt.bar(r2, recall, color='g', width=bar_width, edgecolor='grey', label='Recall')
plt.bar(r3, f1_score, color='r', width=bar_width, edgecolor='grey', label='F1-score')

# Add xticks on the middle of the group bars
plt.xlabel('Class', fontweight='bold')
plt.xticks([r + bar_width for r in range(len(precision))], classes, rotation=45)

# Add y label
plt.ylabel('Scores', fontweight='bold')

# Create legend & Show graphic
plt.legend()
plt.title('Classification Metrics by Class')
plt.tight_layout()
plt.show()


# Save it to a file using joblib
joblib.dump(re_model, 'data/MLmodel/trained_model.pkl')





# Load the model
model = joblib.load('data/MLmodel/best_model.pkl')

# Read the new data
df = pd.read_csv('data/raw/recipes.csv')


# Drop all columns that is not required for analysis
new_data = df.drop(["AuthorName", "AuthorId", "DatePublished", "CookTime", "PrepTime"], axis=1)

# fill NA in rating with 0 for 0 review
new_data['AggregatedRating'].fillna(0, inplace=True)
new_data['ReviewCount'].fillna(0, inplace=True)

# Splitting the 'RecipeYield' column into two new columns
new_data[['Serving', 'Unit']] = new_data['RecipeYield'].str.split(' ', n=1, expand=True)

# Converting 'Serving' column to integer
new_data['Serving'] = new_data['Serving'].str.extract('(\d+)').astype(float)

# Dropping the original 'RecipeYield' column and the 'Unit' column
new_data.drop(['RecipeYield', 'Unit'], axis=1, inplace=True)

# Fill NA in "RecipeServings" with "Serving" value where "RecipeServings" is NA and "Serving" is not null
new_data.loc[new_data['RecipeServings'].isna() & new_data['Serving'].notna(), 'RecipeServings'] = new_data['Serving']

# Convert 'RecipeServings' column to integer type if necessary
new_data['RecipeServings'] = new_data['RecipeServings'].astype(float)

# Fill NA in "Keywords" with values from "RecipeIngredientParts"
new_data['Keywords'].fillna(new_data['RecipeIngredientParts'], inplace=True)

new_data = new_data.drop(["Serving"], axis=1)

# Fill NA in "RecipeServings" with 1
new_data['RecipeServings'].fillna(1, inplace=True)

# Remove rows with NA in "RecipeCategory"
new_data = new_data.dropna(subset=['RecipeCategory', 'Description', 'RecipeIngredientQuantities'])

# Remove "PT" prefix
new_data['TotalTime'] = new_data['TotalTime'].str.replace("PT", "")

# Remove "c" at the beginning and parentheses from required columns
new_data['Keywords'] = new_data['Keywords'].str.replace(r'^c|[()]', '', regex=True)
new_data['RecipeIngredientQuantities'] = new_data['RecipeIngredientQuantities'].str.replace(r'^c|[()]', '', regex=True)
new_data['RecipeIngredientParts'] = new_data['RecipeIngredientParts'].str.replace(r'^c|[()]', '', regex=True)
new_data['RecipeInstructions'] = new_data['RecipeInstructions'].str.replace(r'^c|[()]', '', regex=True)
new_data['Images'] = new_data['Images'].str.replace(r'^c|[()]', '', regex=True)


# Create a copy of new_data
# Drop all columns with value = stings
test_data = new_data.copy()

# Columns to drop
columns_to_drop = ['Name', 'TotalTime', 'Description', 'Images', 'RecipeCategory', 
                   'Keywords', 'RecipeIngredientQuantities', 'RecipeIngredientParts', 
                   'AggregatedRating', 'ReviewCount', 'RecipeInstructions']

# Drop the columns from the DataFrame
test_data.drop(columns=columns_to_drop, inplace=True)


# Make predictions
predictions = model.predict(test_data)

# Add keywords based on predictions
predicted_keywords = ['low sodium', 'low calorie', 'high protein', 'low fat', 'high fat', 'high fiber', 'low carb', 'low cholesterol', 'low sugar']

# Initialize an empty list to store the keywords for each record
keywords_added = []

# Count occurrences of each keyword
keyword_counts = {keyword: 0 for keyword in predicted_keywords}

for pred in predictions:
    # Initialize an empty list to store keywords for the current record
    keywords = []
    for i, val in enumerate(pred):
        # If the value is true (1), append the corresponding keyword
        if val == 1:
            keyword = predicted_keywords[i]
            keywords.append(keyword)
            # Increment the count for this keyword
            keyword_counts[keyword] += 1
    # Append the list of keywords for the current record to the overall list
    keywords_added.append(keywords)

# Print keyword counts
print("Keyword Counts:")
for keyword, count in keyword_counts.items():
    print(f"{keyword}: {count}")


# Add keywords to the new_data DataFrame
new_data['test_Keywords'] = keywords_added


new_data['test_Keywords'].head()


# Convert non-string values to strings, then split them into lists of keywords
new_data['test_Keywords'] = new_data['test_Keywords'].astype(str).str.lower().str.split(',')

# Iterate over each row and append the keywords from 'test_Keywords' to 'Keywords'
for index, row in new_data.iterrows():
    keywords = row['test_Keywords']
    if isinstance(keywords, list):
        # Enclose each keyword in double quotes and join keywords into a single string
        keywords = ', '.join(['"' + keyword.strip("[]' ") + '"' for keyword in keywords])
        new_data.at[index, 'Keywords'] += ", " + keywords

# Drop the 'test_Keywords' column if needed
new_data.drop('test_Keywords', axis=1, inplace=True)


print(new_data['Keywords'].iloc[0])


# Create a new column "URL" with the initial value from "Name" and "RecipeId" column
# Food.com url example "https://www.food.com/recipe/japanese-pumpkin-soup-kabocha-soup-88935"

#Create new column URL to store generated URL Link
new_data['URL'] = new_data['Name']

# Remove parentheses, dashes, and commas from the "URL" column
new_data['URL'] = new_data['URL'].str.replace(r'[\(\)\-,]', '')  

# Select the "URL" column and apply transformations
new_data['URL'] = new_data['URL'].str.replace(' ', '-')  # Replace spaces with hyphens
new_data['URL'] = new_data['URL'].str.lower()  # Convert text to lowercase

# Concatenate "https://www.food.com/recipe/" with the modified "URL" column
new_data['URL'] = 'https://www.food.com/recipe/' + new_data['URL'] + '-' + new_data['RecipeId'].astype(str)
new_data['URL']


print(new_data['Keywords'].iloc[0])


# Save clean data to a new CSV file
new_data.to_csv("data/clean/ML_generated_recipes.csv", index=False)


# Save clean data to a new JSON file
new_data.to_json("data/clean/cleanMLrecipes.json", orient="records")



