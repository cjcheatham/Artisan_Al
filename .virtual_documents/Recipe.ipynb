import pandas as pd
import re


# Read the CSV file into a DataFrame
df = pd.read_csv("data/raw/recipes_sample.csv")


# print all columns and verify if any missing values
df.info()


# Drop all columns that is not required for analysis
clean_df = df.drop(["AuthorName", "AuthorId", "DatePublished", "CookTime", "PrepTime"], axis=1)


# fill NA in rating with 0 for 0 review
clean_df['AggregatedRating'].fillna(0, inplace=True)
clean_df['ReviewCount'].fillna(0, inplace=True)


# Splitting the 'RecipeYield' column into two new columns
clean_df[['Serving', 'Unit']] = clean_df['RecipeYield'].str.split(' ', n=1, expand=True)

# Converting 'Serving' column to integer
clean_df['Serving'] = clean_df['Serving'].str.extract('(\d+)').astype(float)

# Dropping the original 'RecipeYield' column and the 'Unit' column
clean_df.drop(['RecipeYield', 'Unit'], axis=1, inplace=True)


# Fill NA in "RecipeServings" with "Serving" value where "RecipeServings" is NA and "Serving" is not null
clean_df.loc[clean_df['RecipeServings'].isna() & clean_df['Serving'].notna(), 'RecipeServings'] = clean_df['Serving']

# Convert 'RecipeServings' column to integer type if necessary
clean_df['RecipeServings'] = clean_df['RecipeServings'].astype(float)


# Fill NA in "Keywords" with values from "RecipeIngredientParts"
clean_df['Keywords'].fillna(clean_df['RecipeIngredientParts'], inplace=True)


clean_df = clean_df.drop(["Serving"], axis=1)


# Fill NA in "RecipeServings" with 1
clean_df['RecipeServings'].fillna(1, inplace=True)


# Remove rows with NA in "RecipeCategory"
clean_df = clean_df.dropna(subset=['RecipeCategory', 'Description', 'RecipeIngredientQuantities'])

# Veirfy all column value are cleaned
clean_df.info()


# Remove "PT" prefix
clean_df['TotalTime'] = clean_df['TotalTime'].str.replace("PT", "")
clean_df.head()


# Create a new column "URL" with the initial value from "Name" and "RecipeId" column
# Food.com url example "https://www.food.com/recipe/japanese-pumpkin-soup-kabocha-soup-88935"

#Create new column URL to store generated URL Link
clean_df['URL'] = clean_df['Name']

# Remove parentheses, dashes, and commas from the "URL" column
clean_df['URL'] = clean_df['URL'].str.replace(r'[\(\)\-,]', '')  

# Select the "URL" column and apply transformations
clean_df['URL'] = clean_df['URL'].str.replace(' ', '-')  # Replace spaces with hyphens
clean_df['URL'] = clean_df['URL'].str.lower()  # Convert text to lowercase

# Concatenate "https://www.food.com/recipe/" with the modified "URL" column
clean_df['URL'] = 'https://www.food.com/recipe/' + clean_df['URL'] + '-' + clean_df['RecipeId'].astype(str)
clean_df['URL']


# Remove "c()" at the beginning and parentheses from each value
clean_df['Keywords'] = clean_df['Keywords'].str.replace(r'^c|[()]', '', regex=True)
clean_df['RecipeIngredientQuantities'] = clean_df['RecipeIngredientQuantities'].str.replace(r'^c|[()]', '', regex=True)
clean_df['RecipeIngredientParts'] = clean_df['RecipeIngredientParts'].str.replace(r'^c|[()]', '', regex=True)
clean_df['RecipeInstructions'] = clean_df['RecipeInstructions'].str.replace(r'^c|[()]', '', regex=True)
clean_df['Images'] = clean_df['Images'].str.replace(r'^c|[()]', '', regex=True)


# Split the strings by comma, explode the resulting lists, and get unique items
keyword_item = clean_df['Keywords'].str.split(',').explode().str.strip().unique()

# Convert to list
keyword_item_list = keyword_item.tolist()

# Print the length of the list
print("Number of unique items:", len(keyword_item_list))


# Create unique_keyword_item to understand what is in the Keywords column for future programming use
# Clean up each string (remove leading/trailing spaces, convert to lowercase, and strip double quotes) before converting to set
unique_keyword_item_set = set(map(lambda x: x.strip().lower().strip('"'), keyword_item_list))

# Convert the set back to a list if needed
unique_keyword_item_list = list(unique_keyword_item_set)

# Print the length of the list
print("Number of unique items:", len(unique_keyword_item_list))


# Verify all data are cleaned properly 
clean_df.head()


# Extract the recipe ingredients from the "RecipeIngredientParts" column
ingredients_series = clean_df['RecipeIngredientParts']

# Split the ingredients into individual items
ingredients_split = ingredients_series.str.split(',')

# Create a list of unique ingredients
unique_ingredients = []
for ingredients in ingredients_split:
    if isinstance(ingredients, list):
        for ingredient in ingredients:
            ingredient = ingredient.strip()
            if ingredient not in unique_ingredients and ingredient != '':
                unique_ingredients.append(ingredient)


# Create a DataFrame with columns representing unique ingredients
ingredient_df = pd.DataFrame(0, index=clean_df.index, columns=unique_ingredients)

# Populate the DataFrame with 1 where ingredient is present in the recipe
for ingredient in unique_ingredients:
    ingredient_df[ingredient] = ingredients_series.str.contains(ingredient, na=False, regex=False).astype(int)



# Create new table call recipes with clean_df and ingredient_df for ML features to learn
recipes = pd.concat([clean_df, ingredient_df], axis=1)


# Melt the DataFrame to convert ingredient columns into rows
melted_df = recipes.melt(var_name='Ingredient', value_name='Present')

# Filter out rows where ingredient is present
present_ingredients = melted_df[melted_df['Present'] == 1]

# Count the occurrences of each ingredient
ingredient_counts = present_ingredients['Ingredient'].value_counts().reset_index()

# Rename columns
ingredient_counts.columns = ['Ingredient', 'Count']


# Define the thresholds for each category to match each Healthy category
category_thresholds = {
    'Low Sodium': 140,    # Daily recommended intake for low sodium
    'Low Calorie': 200,   # Arbitrary threshold for low calorie
    'High Protein': 20,   # Arbitrary threshold for high protein
    'Low Fat': 3,         # Arbitrary threshold for low fat
    'High Fat': 20,       # Arbitrary threshold for high fat
    'High Fiber': 5,      # Arbitrary threshold for high fiber
    'Low Carb': 20,       # Arbitrary threshold for low carb
    'Low Cholesterol': 20, # Arbitrary threshold for low cholesterol
    'Low Sugar': 5        # Arbitrary threshold for low sugar
}


# Create new columns for each category and assign boolean values based on thresholds
for category, threshold in category_thresholds.items():
    recipes[category] = False
    if category == 'Low Sodium':
        recipes.loc[recipes['SodiumContent'] < threshold, category] = True
    elif category == 'Low Calorie':
        recipes.loc[recipes['Calories'] < threshold, category] = True
    elif category == 'High Protein':
        recipes.loc[recipes['ProteinContent'] > threshold, category] = True
    elif category == 'Low Fat':
        recipes.loc[recipes['FatContent'] < threshold, category] = True
    elif category == 'High Fat':
        recipes.loc[recipes['FatContent'] > threshold, category] = True
    elif category == 'high fiber':
        recipes.loc[recipes['FiberContent'] > threshold, category] = True
    elif category == 'Low Carb':
        recipes.loc[recipes['CarbohydrateContent'] < threshold, category] = True
    elif category == 'Low Cholesterol':
        recipes.loc[recipes['CholesterolContent'] < threshold, category] = True
    elif category == 'Low Sugar':
        recipes.loc[recipes['SugarContent'] < threshold, category] = True


# Drop all columns except the newly created category columns
columns_to_keep = ["Name"] + list(category_thresholds.keys())

# make new copy table recipes2 to store binery of each category_thresholds
recipes2 = recipes[columns_to_keep]


# Function to determine the category for each recipe
def determine_category(row):
    for category, threshold in category_thresholds.items():
        if category == 'Low Sodium':
            if row['SodiumContent'] < threshold:
                return category
        elif category == 'Low Calorie':
            if row['Calories'] < threshold:
                return category
        elif category == 'High Protein':
            if row['ProteinContent'] > threshold:
                return category
        elif category == 'Low Fat':
            if row['FatContent'] < threshold:
                return category
        elif category == 'High Fat':
            if row['FatContent'] > threshold:
                return category
        elif category == 'High Fiber':
            if row['FiberContent'] > threshold:
                return category
        elif category == 'Low Carb':
            if row['CarbohydrateContent'] < threshold:
                return category
        elif category == 'Low Cholesterol':
            if row['CholesterolContent'] < threshold:
                return category
        elif category == 'Low Sugar':
            if row['SugarContent'] < threshold:
                return category
    # If no category is assigned, return None
    return None

# Apply the determine_category function to each row to determine the category for each recipe and add it to the recipes table
recipes['AssignedCategory'] = recipes.apply(determine_category, axis=1)


# Verify all Category are properly appended
recipes.head()


# Adding Health Keywords to Keywords column
# Define the criteria for adding keywords
criteria = (
    recipes['FiberContent'] > 5
) | (
    recipes['RecipeCategory'].str.contains('High In', na=False)
) | (
    recipes['ProteinContent'] > 20
) | (
    recipes['RecipeCategory'].str.contains('Homeopathy/Remedies', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Kid Friendly', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Lactose Free', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Low Cholesterol', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Low Protein', na=False)
) | (
    recipes['RecipeCategory'].str.contains('No Shell Fish', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Soy/Tofu', na=False)
) | (
    recipes['CarbohydrateContent'] < 5
) | (
    recipes['RecipeCategory'].str.contains('High In.*Diabetic Friendly', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Bath/Beauty', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Egg Free', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Healthy', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Kosher', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Toddler Friendly', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Vegan', na=False)
) | (
    recipes['RecipeCategory'].str.contains('Vegetable', na=False)
)


# Add the specified keywords to the Keywords column for recipes that meet the criteria
clean_df.loc[criteria, 'Keywords'] = recipes.loc[criteria, 'Keywords'] + ', "High Fiber", "High Protein", "Homeopathy/Remedies", "Kid Friendly", "Lactose Free", "Low Cholesterol", "Low Protein", "No Shell Fish", "Soy/Tofu", "Very Low Carbs", "Diabetic Friendly", "Bath/Beauty", "Egg Free", "Healthy", "Kosher", "Toddler Friendly", "Vegan", "Vegetable"'


# Get the column names of ingredient_df
ingredient_columns = ingredient_df.columns.tolist()

# Assign ingredient_columns to columns_to_drop
columns_to_drop = ingredient_columns

# Dropping the columns from the DataFrame
recipes.drop(columns=columns_to_drop, inplace=True)


# Create a new DataFrame named description_df[] from new_data
training_df = recipes[["RecipeId", "Calories", "FatContent", "SaturatedFatContent","CholesterolContent", 
                           "SodiumContent", "CarbohydrateContent", "FiberContent","SugarContent",
                           "ProteinContent", "RecipeServings", "Low Sodium", "Low Calorie","High Protein",
                           "Low Fat", "High Fat", "High Fiber", "Low Carb","Low Cholesterol", "Low Sugar"
                           ]].copy()
 
# Save it to a CSV file
training_df.info()


# Create a sample_training_data as CSV file for ML model testing
training_df.to_csv("data/clean/sample_training_data.csv", index=False)





# Importing necessary libraries
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
import matplotlib.pyplot as plt
import numpy as np
import joblib
import time
from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder \
    .appName("ML Training") \
    .getOrCreate()

# Load data from CSV using Spark
df = spark.read.csv("data/clean/sample_training_data.csv", header=True, inferSchema=True)

# Convert Spark DataFrame to Pandas DataFrame
ML_test_df = df.toPandas()

# Stop the SparkSession
spark.stop()


ML_test_df.head()


# Define features (X) and target variable (y)
# Features
X = ML_test_df.drop(columns=['Low Sodium', 'Low Calorie', 'High Protein', 'Low Fat', 'High Fat',	
                             'High Fiber', 'Low Carb', 'Low Cholesterol', 'Low Sugar'])
# Labels for multiple health attributes
y = ML_test_df[['Low Sodium', 'Low Calorie', 'High Protein', 'Low Fat',	'High Fat',	'High Fiber', 
                'Low Carb', 'Low Cholesterol', 'Low Sugar']]  # Target variable

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Model Selection with Hyperparameter Tuning using GridSearchCV
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_


# Start the timer
start_time = time.time()

# Training the Best Model
best_model.fit(X_train, y_train)


# End the timer and calculate the duration
training_duration = time.time() - start_time

# Make predictions
y_pred = best_model.predict(X_test)

# # Model Evaluation
print("Training duration:", training_duration, "seconds")
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Calculating the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy of Random Forest classifier:", accuracy)


# Class labels
classes = ['Low Sodium', 'Low Calorie', 'High Protein', 'Low Fat', 'High Fat', 
           'High Fiber', 'Low Carb', 'Low Cholesterol', 'Low Sugar']

# Precision, recall, and F1-score values for each class
precision = [0.99, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00]
recall = [0.95, 0.99, 0.99, 0.88, 0.98, 0.91, 0.99, 1.00, 1.00]
f1_score = [0.97, 0.99, 0.99, 0.94, 0.99, 0.95, 0.99, 1.00, 1.00]

# Set width of bar
bar_width = 0.25

# Set position of bar on X axis
r1 = np.arange(len(precision))
r2 = [x + bar_width for x in r1]
r3 = [x + bar_width for x in r2]

# Make the plot
plt.figure(figsize=(12, 6))
plt.bar(r1, precision, color='b', width=bar_width, edgecolor='grey', label='Precision')
plt.bar(r2, recall, color='g', width=bar_width, edgecolor='grey', label='Recall')
plt.bar(r3, f1_score, color='r', width=bar_width, edgecolor='grey', label='F1-score')

# Add xticks on the middle of the group bars
plt.xlabel('Class', fontweight='bold')
plt.xticks([r + bar_width for r in range(len(precision))], classes, rotation=45)

# Add y label
plt.ylabel('Scores', fontweight='bold')

# Create legend & Show graphic
plt.legend()
plt.title('Classification Metrics by Class')
plt.tight_layout()
plt.show()


# Save the best model to a file
joblib.dump(best_model, 'data/MLmodel/best_model.pkl')





import joblib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
# Load the model
model = joblib.load('data/MLmodel/best_model.pkl')

# Read the new data
df = pd.read_csv('data/raw/recipes.csv')
df.info()


# Drop all columns that is not required for analysis
new_data = df.drop(["AuthorName", "AuthorId", "DatePublished", "CookTime", "PrepTime"], axis=1)

# Drop rows with NA values in 'AggregatedRating' and 'ReviewCount' columns where no reviews
new_data.dropna(subset=['AggregatedRating', 'ReviewCount'], inplace=True)

# Splitting the 'RecipeYield' column into two new columns
new_data[['Serving', 'Unit']] = new_data['RecipeYield'].str.split(' ', n=1, expand=True)

# Converting 'Serving' column to integer
new_data['Serving'] = new_data['Serving'].str.extract('(\d+)').astype(float)

# Dropping the original 'RecipeYield' column and the 'Unit' column
new_data.drop(['RecipeYield', 'Unit'], axis=1, inplace=True)

# Fill NA in "RecipeServings" with "Serving" value where "RecipeServings" is NA and "Serving" is not null
new_data.loc[new_data['RecipeServings'].isna() & new_data['Serving'].notna(), 'RecipeServings'] = new_data['Serving']

# Convert 'RecipeServings' column to integer type if necessary
new_data['RecipeServings'] = new_data['RecipeServings'].astype(float)

# Fill NA in "Keywords" with values from "RecipeIngredientParts"
new_data['Keywords'].fillna(new_data['RecipeIngredientParts'], inplace=True)

new_data = new_data.drop(["Serving"], axis=1)

# Fill NA in "RecipeServings" with 1
new_data['RecipeServings'].fillna(1, inplace=True)

# Remove rows with NA in "RecipeCategory"
new_data = new_data.dropna(subset=['RecipeCategory', 'Description', 'RecipeIngredientQuantities'])

# Remove "PT" prefix
new_data['TotalTime'] = new_data['TotalTime'].str.replace("PT", "")

# Remove "c" at the beginning and parentheses from required columns
new_data['Keywords'] = new_data['Keywords'].str.replace(r'^c|[()]', '', regex=True)
new_data['RecipeIngredientQuantities'] = new_data['RecipeIngredientQuantities'].str.replace(r'^c|[()]', '', regex=True)
new_data['RecipeIngredientParts'] = new_data['RecipeIngredientParts'].str.replace(r'^c|[()]', '', regex=True)
new_data['RecipeInstructions'] = new_data['RecipeInstructions'].str.replace(r'^c|[()]', '', regex=True)
new_data['Images'] = new_data['Images'].str.replace(r'^c|[()]', '', regex=True)

# Rename column 'top ingredient'
new_data.rename(columns={'top ingredient': 'TopIngredient'}, inplace=True)





# List of keywords to match
keywords_to_match = ['Low Sodium', 'Low Calorie', 'High Protein', 'Low Fat', 'High Fat', 
                     'High Fiber', 'Low Carb', 'Low Cholesterol', 'Low Sugar']

# Initialize a dictionary to store counts
keyword_counts = {keyword: 0 for keyword in keywords_to_match}

# Iterate over each row in the DataFrame
for row_keywords in new_data['Keywords']:
    # Split the string by commas and remove leading/trailing whitespace and double quotes
    keywords = [keyword.strip().strip('"') for keyword in row_keywords.split(',')]
    # Count occurrences of each keyword
    for keyword in keywords:
        if keyword in keywords_to_match:
            keyword_counts[keyword] += 1

# Print the counts
for keyword, count in keyword_counts.items():
    print(f"{keyword}: {count}")


# Plotting the bar chart
plt.figure(figsize=(5, 4))  # Adjust the figsize as needed
plt.bar(keyword_counts.keys(), keyword_counts.values(), color='skyblue')
plt.ylabel('Counts')
plt.title('Counts of Keywords before ML Append')
plt.xticks(rotation=90)
plt.tight_layout()  # Add tight layout
plt.show()


# Save clean data to a new CSV file
new_data.to_csv("data/clean/clean_recipes.csv", index=False)


# Create a copy of new_data
# Drop all columns with value = stings
test_data = new_data.copy()

# Columns to drop
columns_to_drop = ['Name', 'TotalTime', 'Description', 'Images', 'RecipeCategory', 
                   'Keywords', 'RecipeIngredientQuantities', 'RecipeIngredientParts', 
                   'AggregatedRating', 'ReviewCount', 'RecipeInstructions']

# Drop the columns from the DataFrame
test_data.drop(columns=columns_to_drop, inplace=True)


# Make predictions
predictions = model.predict(test_data)

# Add keywords based on predictions
predicted_keywords = ['Low Sodium', 'Low Calorie', 'High Protein', 'Low Fat', 'High Fat', 
                      'High Fiber', 'Low Carb', 'Low Cholesterol', 'Low Sugar']

# Initialize an empty list to store the keywords for each record
keywords_added = []

# Count occurrences of each keyword
keyword_counts = {keyword: 0 for keyword in predicted_keywords}

for pred in predictions:
    # Initialize an empty list to store keywords for the current record
    keywords = []
    for i, val in enumerate(pred):
        # If the value is true (1), append the corresponding keyword
        if val == 1:
            keyword = predicted_keywords[i]
            keywords.append(keyword)
            # Increment the count for this keyword
            keyword_counts[keyword] += 1
    # Append the list of keywords for the current record to the overall list
    keywords_added.append(keywords)

# Print keyword counts
print("Keyword Counts:")
for keyword, count in keyword_counts.items():
    print(f"{keyword}: {count}")


# Add keywords to the new_data DataFrame
new_data['test_Keywords'] = keywords_added


new_data['test_Keywords'].head()


# Convert non-string values to strings, then split them into lists of keywords
new_data['test_Keywords'] = new_data['test_Keywords'].astype(str).str.split(',')

# Iterate over each row and append the keywords from 'test_Keywords' to 'Keywords'
for index, row in new_data.iterrows():
    keywords = row['test_Keywords']
    if isinstance(keywords, list):
        # Enclose each keyword in double quotes and join keywords into a single string
        keywords = ', '.join(['"' + keyword.strip("[]' ") + '"' for keyword in keywords])
        new_data.at[index, 'Keywords'] += ", " + keywords

# Drop the 'test_Keywords' column if needed
new_data.drop('test_Keywords', axis=1, inplace=True)


print(new_data['Keywords'].iloc[0])


# Create a new column "URL" with the initial value from "Name" and "RecipeId" column
# Food.com url example "https://www.food.com/recipe/japanese-pumpkin-soup-kabocha-soup-88935"

#Create new column URL to store generated URL Link
new_data['URL'] = new_data['Name']

# Remove parentheses, dashes, and commas from the "URL" column
new_data['URL'] = new_data['URL'].str.replace(r'[\(\)\-,]', '')  

# Select the "URL" column and apply transformations
new_data['URL'] = new_data['URL'].str.replace(' ', '-')  # Replace spaces with hyphens
new_data['URL'] = new_data['URL'].str.lower()  # Convert text to lowercase

# Concatenate "https://www.food.com/recipe/" with the modified "URL" column
new_data['URL'] = 'https://www.food.com/recipe/' + new_data['URL'] + '-' + new_data['RecipeId'].astype(str)
new_data['URL']





# List of keywords to match
keywords_to_match = ['Low Sodium', 'Low Calorie', 'High Protein', 'Low Fat', 'High Fat', 
                      'High Fiber', 'Low Carb', 'Low Cholesterol', 'Low Sugar']

# Initialize a dictionary to store counts
keyword_counts = {keyword: 0 for keyword in keywords_to_match}

# Iterate over each row in the DataFrame
for row_keywords in new_data['Keywords']:
    # Split the string by commas and remove leading/trailing whitespace and double quotes
    keywords = [keyword.strip().strip('"') for keyword in row_keywords.split(',')]
    # Count occurrences of each keyword
    for keyword in keywords:
        if keyword in keywords_to_match:
            keyword_counts[keyword] += 1

# Print the counts
for keyword, count in keyword_counts.items():
    print(f"{keyword}: {count}")


# Plotting the bar chart
plt.figure(figsize=(5, 4))  # Adjust the figsize as needed
plt.bar(keyword_counts.keys(), keyword_counts.values(), color='skyblue')
plt.ylabel('Counts')
plt.title('Counts of Keywords after ML Append')
plt.xticks(rotation=90)
plt.tight_layout()  # Add tight layout
plt.show()


# Check new_data information
new_data.info()


# Check new_data information
new_data.info()


# Save clean data to a new JSON file
new_data.to_json("data/clean/cleanMLrecipes.json", orient="records")





import matplotlib.pyplot as plt
import numpy as np


# Read the new data
df = pd.read_csv('data/raw/recipes.csv')


# Drop all columns that is not required for analysis
new_data = df.drop(["AuthorName", "AuthorId", "DatePublished", "CookTime", "PrepTime"], axis=1)

# Drop rows with NA values in 'AggregatedRating' and 'ReviewCount' columns where no reviews
# new_data.dropna(subset=['AggregatedRating', 'ReviewCount'], inplace=True)

# Fill NaN values in 'AggregatedRating' and 'ReviewCount' columns with 0
new_data['AggregatedRating'].fillna(0, inplace=True)
new_data['ReviewCount'].fillna(0, inplace=True)

# Splitting the 'RecipeYield' column into two new columns
new_data[['Serving', 'Unit']] = new_data['RecipeYield'].str.split(' ', n=1, expand=True)

# Converting 'Serving' column to integer
new_data['Serving'] = new_data['Serving'].str.extract('(\d+)').astype(float)

# Dropping the original 'RecipeYield' column and the 'Unit' column
new_data.drop(['RecipeYield', 'Unit'], axis=1, inplace=True)

# Fill NA in "RecipeServings" with "Serving" value where "RecipeServings" is NA and "Serving" is not null
new_data.loc[new_data['RecipeServings'].isna() & new_data['Serving'].notna(), 'RecipeServings'] = new_data['Serving']

# Convert 'RecipeServings' column to integer type if necessary
new_data['RecipeServings'] = new_data['RecipeServings'].astype(float)

# Fill NA in "Keywords" with values from "RecipeIngredientParts"
new_data['Keywords'].fillna(new_data['RecipeIngredientParts'], inplace=True)

new_data = new_data.drop(["Serving"], axis=1)

# Fill NA in "RecipeServings" with 1
new_data['RecipeServings'].fillna(1, inplace=True)

# Remove rows with NA in "RecipeCategory"
new_data = new_data.dropna(subset=['RecipeCategory', 'Description', 'RecipeIngredientQuantities'])

# Remove "PT" prefix
new_data['TotalTime'] = new_data['TotalTime'].str.replace("PT", "")

# Remove "c" at the beginning and parentheses from required columns
new_data['Keywords'] = new_data['Keywords'].str.replace(r'^c|[()]', '', regex=True)
new_data['RecipeIngredientQuantities'] = new_data['RecipeIngredientQuantities'].str.replace(r'^c|[()]', '', regex=True)
new_data['RecipeIngredientParts'] = new_data['RecipeIngredientParts'].str.replace(r'^c|[()]', '', regex=True)
new_data['RecipeInstructions'] = new_data['RecipeInstructions'].str.replace(r'^c|[()]', '', regex=True)
new_data['Images'] = new_data['Images'].str.replace(r'^c|[()]', '', regex=True)


# List of keywords to match
keywords_to_match = ['Low Sodium', 'Low Calorie', 'High Protein', 'Low Fat', 'High Fat', 
                     'High Fiber', 'Low Carb', 'Low Cholesterol', 'Low Sugar']

# Initialize a dictionary to store counts
keyword_counts = {keyword: 0 for keyword in keywords_to_match}

# Iterate over each row in the DataFrame
for row_keywords in new_data['Keywords']:
    # Split the string by commas and remove leading/trailing whitespace and double quotes
    keywords = [keyword.strip().strip('"') for keyword in row_keywords.split(',')]
    # Count occurrences of each keyword
    for keyword in keywords:
        if keyword in keywords_to_match:
            keyword_counts[keyword] += 1

# Print the counts
for keyword, count in keyword_counts.items():
    print(f"{keyword}: {count}")


# Plotting the bar chart
plt.figure(figsize=(5, 4))  # Adjust the figsize as needed
plt.bar(keyword_counts.keys(), keyword_counts.values(), color='skyblue')
plt.ylabel('Counts')
plt.title('Counts of Keywords before ML Append')
plt.xticks(rotation=90)
plt.tight_layout()  # Add tight layout
plt.show()





# Keywords to consider
keywords_to_consider = ['Low Sodium', 'Low Calorie', 'High Protein', 'Low Fat', 'High Fat', 
                      'High Fiber', 'Low Carb', 'Low Cholesterol', 'Low Sugar']


# Initialize empty lists for each keyword
keywords_data = {
    'Low Sodium': [],
    'Low Calorie': [],
    'High Protein': [],
    'Low Fat': [],
    'High Fat': [],
    'High Fiber': [],
    'Low Carb': [],
    'Low Cholesterol': [],
    'Low Sugar': []    
}

# Iterate through each row of the DataFrame
for index, row in new_data.iterrows():
    keywords = [keyword.strip().strip('"') for keyword in row['Keywords'].split(',')]

    # Iterate through each keyword and append the record index to the respective keyword list
    for keyword in keywords:
        if keyword in keywords_to_consider:
            keywords_data[keyword].append(index)

# Perform calculations for each keyword
for keyword, records in keywords_data.items():
    print(f"Keyword: {keyword}")
    print(f"Number of records found in dataset: {len(records)}")
    
    records_with_reviews = [record for record in records if new_data.loc[record, 'ReviewCount'] > 0]
    records_without_reviews = [record for record in records if new_data.loc[record, 'ReviewCount'] == 0]
    
    print(f"Number of records with reviews: {len(records_with_reviews)}")
    print(f"Number of records without reviews: {len(records_without_reviews)}")
    
    total_review_count = sum(new_data.loc[records, 'ReviewCount'].fillna(0))
    print(f"Total review count: {total_review_count}")
    
    total_rating = sum(new_data.loc[records, 'AggregatedRating'].fillna(0))
    average_rating = total_rating / len(records_with_reviews) if len(records_with_reviews) > 0 else 0
    average_rating_rounded = round(average_rating, 2)
    print(f"Total rating: {total_rating}")
    print(f"Average rating (only include records with reviews): {average_rating_rounded}")
    
    print()





# Keywords to consider
keywords_to_exclude = ['Low Sodium', 'Low Calorie', 'High Protein', 'Low Fat', 'High Fat', 
                      'High Fiber', 'Low Carb', 'Low Cholesterol', 'Low Sugar']

# Initialize an empty list for records without specified keywords
records_without_keywords = []

# Iterate through each row of the DataFrame
for index, row in new_data.iterrows():
    keywords = [keyword.strip().strip('"') for keyword in row['Keywords'].split(',')]
    contains_keyword = any(keyword in keywords for keyword in keywords_to_exclude)

    if not contains_keyword:
        records_without_keywords.append(index)

# Perform calculations for records without specified keywords
print("Records without specified Health Keywords:")
print(f"Number of records found in dataset: {len(records_without_keywords)}")

# Number of records with reviews among records without specified keywords
records_with_reviews = new_data.loc[records_without_keywords][new_data.loc[records_without_keywords]['ReviewCount'] > 0]
print(f"Number of records with reviews: {len(records_with_reviews)}")

# Number of records without reviews among records without specified keywords
records_without_reviews = new_data.loc[records_without_keywords][new_data.loc[records_without_keywords]['ReviewCount'] == 0]
print(f"Number of records without reviews: {len(records_without_reviews)}")

total_review_count = sum(new_data.loc[records_without_keywords, 'ReviewCount'].fillna(0))
print(f"Total review count: {total_review_count}")

total_rating = sum(new_data.loc[records_without_keywords, 'AggregatedRating'].fillna(0))
average_rating = total_rating / len(records_with_reviews) if len(records_with_reviews) > 0 else 0
average_rating_rounded = round(average_rating, 2)
print(f"Total rating: {total_rating}")
print(f"Average rating: {average_rating_rounded}")

print()


# Define the data
labels = ['With reviews', 'Without reviews']
sizes = [len(records_with_reviews), len(records_without_reviews)]

# Define the full path for saving the plot
save_path = 'images/without_keywords_reviews_analysis.png'

# Create a pie chart
plt.figure(figsize=(5, 4))
plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)
plt.title('Without Health Keywords Reviews Analysis')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Define text box properties
textbox_props = dict(boxstyle='round', facecolor='white', alpha=0.5)

# Add a text box to display additional information
additional_info = (
    f"Keyword record found: {len(records_without_keywords)}\n"
    f"Records with reviews: {len(records_with_reviews)}\n"
    f"Total review count: {total_review_count}\n"
    f"Average rating: {average_rating_rounded}"
)
plt.text(0.9, -0.8, additional_info, fontsize=9, verticalalignment='center', bbox=textbox_props)

# Save the plot as an image file in the specified directory
plt.savefig(save_path, bbox_inches='tight')

plt.show()


# Keywords to consider
keywords_to_consider = ['Low Sodium', 'Low Calorie', 'High Protein', 'Low Fat', 'High Fat', 
                      'High Fiber', 'Low Carb', 'Low Cholesterol', 'Low Sugar']

# Initialize empty lists for each keyword
keywords_data = {
    'Low Sodium': [],
    'Low Calorie': [],
    'High Protein': [],
    'Low Fat': [],
    'High Fat': [],
    'High Fiber': [],
    'Low Carb': [],
    'Low Cholesterol': [],
    'Low Sugar': []    
}

# Iterate through each row of the DataFrame
for index, row in new_data.iterrows():
    keywords = [keyword.strip().strip('"') for keyword in row['Keywords'].split(',')]

    # Iterate through each keyword and append the record index to the respective keyword list
    for keyword in keywords:
        if keyword in keywords_to_consider:
            keywords_data[keyword].append(index)

# Perform calculations for each keyword
total_records_with_keywords = 0
total_records_with_reviews = 0
total_records_without_reviews = 0
total_review_count = 0
total_rating = 0

for keyword, records in keywords_data.items():
    total_records_with_keywords += len(records)
    
    records_with_reviews = [record for record in records if new_data.loc[record, 'ReviewCount'] > 0]
    records_without_reviews = [record for record in records if new_data.loc[record, 'ReviewCount'] == 0]
    
    total_records_with_reviews += len(records_with_reviews)
    total_records_without_reviews += len(records_without_reviews)
    
    total_review_count += sum(new_data.loc[records, 'ReviewCount'].fillna(0))
    total_rating += sum(new_data.loc[records, 'AggregatedRating'].fillna(0))

average_rating = total_rating / total_records_with_reviews if total_records_with_reviews > 0 else 0
average_rating_rounded = round(average_rating, 2)

print("Total Number of records with Keywords in dataset:", total_records_with_keywords)
print("Number of records with reviews:", total_records_with_reviews)
print("Number of records without reviews:", total_records_without_reviews)
print("Total review count:", total_review_count)
print("Total rating:", total_rating)
print("Average rating:", average_rating_rounded)


# Define the data
labels = ['With reviews', 'Without reviews']
sizes = [total_records_with_reviews, total_records_without_reviews]

# Define the full path for saving the plot
save_path = 'images/with_keywords_reviews_analysis.png'

# Create a pie chart
plt.figure(figsize=(5, 4))
plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)
plt.title('Health Keywords Reviews Analysis')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Define text box properties
textbox_props = dict(boxstyle='round', facecolor='white', alpha=0.5)

# Add a text box to display additional information
additional_info = (
    f"Keyword record found: {total_records_with_keywords}\n"
    f"Records with reviews: {total_records_with_reviews}\n"
    f"Total review count: {total_review_count}\n"
    f"Average rating: {average_rating_rounded}"
)
plt.text(0.9, -0.8, additional_info, fontsize=9, verticalalignment='center', bbox=textbox_props)

# Save the plot as an image file in the specified directory
plt.savefig(save_path, bbox_inches='tight')

plt.show()



